{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16daba64-0e07-49e7-9a63-85f4e44d18ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\shriharsh\\appdata\\roaming\\python\\python313\\site-packages (2.3.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\shriharsh\\appdata\\roaming\\python\\python313\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\shriharsh\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shriharsh\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shriharsh\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shriharsh\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\shriharsh\\appdata\\roaming\\python\\python313\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shriharsh\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e18942-c08c-4594-b0f3-deafaa636228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd>=2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960f147a-7569-4ae1-8a20-a196a5717d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Smart Conversion Process (Robust Mode with Verification)...\n",
      "üìÇ Scanning current folder: C:\\Users\\Shriharsh\\SIH\n",
      "üìñ Processing Ayurveda (Found: NATIONAL AYURVEDA MORBIDITY CODES.xls)...\n",
      "   [Debug] Found Columns: ['sr no.', 'namc_id', 'namc_code', 'namc_term', 'namc_term_diacritical'] ...\n",
      "   ‚úÖ Added 2910 records from Ayurveda\n",
      "üìñ Processing Siddha (Found: NATIONAL SIDDHA MORBIDITY CODES.xls)...\n",
      "   [Debug] Found Columns: ['sr no.', 'namc_id', 'namc_code', 'namc_term', 'tamil_term'] ...\n",
      "   ‚úÖ Added 1926 records from Siddha\n",
      "üìñ Processing Unani (Found: NATIONAL UNANI MORBIDITY CODES.xls)...\n",
      "   [Debug] Found Columns: ['sr no.', 'numc_id', 'numc_code', 'arabic_term', 'numc_term'] ...\n",
      "   ‚úÖ Added 2522 records from Unani\n",
      "üìñ Processing ICD-10 (Found: NATIONAL ICD10 MORBIDITY CODES.xls)...\n",
      "   [Debug] Found Columns: ['sr no.', 'namc_id', 'namc_code', 'namc_term', 'block_title'] ...\n",
      "   ‚úÖ Added 11145 records from ICD-10\n",
      "\n",
      "===================================================\n",
      "üîç QUALITY CHECK REPORT\n",
      "===================================================\n",
      "‚úÖ Ayurveda: 2910 records, 100% Clean.\n",
      "‚ö†Ô∏è  Siddha: 1926 records, 4 terms missing/unknown.\n",
      "‚ö†Ô∏è  Unani: 2522 records, 2 terms missing/unknown.\n",
      "‚úÖ ICD-10: 11145 records, 100% Clean.\n",
      "===================================================\n",
      "üéâ SUCCESS! Total 18503 records saved to 'namaste_data.json'\n",
      "üëâ You can now run 'server.py' to use this database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "OUTPUT_JSON_FILE = \"namaste_data.json\"\n",
    "\n",
    "# UPDATED: Keys are now all lowercase to match the Normalized Headers logic\n",
    "FILES_CONFIG = [\n",
    "    {\n",
    "        \"system\": \"Ayurveda\",\n",
    "        \"keyword\": \"AYURVEDA\", \n",
    "        \"mapping\": {\n",
    "            \"term\": \"namc_term\",          \n",
    "            \"english\": \"name english\",    \n",
    "            \"tm2_code\": \"namc_code\"       \n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"Siddha\",\n",
    "        \"keyword\": \"SIDDHA\",\n",
    "        \"mapping\": {\n",
    "            \"term\": \"namc_term\",\n",
    "            \"english\": \"short_definition\", \n",
    "            \"tm2_code\": \"namc_code\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"Unani\",\n",
    "        \"keyword\": \"UNANI\",\n",
    "        \"mapping\": {\n",
    "            \"term\": \"numc_term\",\n",
    "            \"english\": \"short_definition\", \n",
    "            \"tm2_code\": \"numc_code\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"ICD-10\",\n",
    "        \"keyword\": \"ICD10\",\n",
    "        \"mapping\": {\n",
    "            \"term\": \"namc_term\",\n",
    "            \"english\": \"namc_term\",       \n",
    "            \"tm2_code\": \"namc_code\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def find_file_by_keyword(keyword):\n",
    "    \"\"\"\n",
    "    Scans the current directory for a CSV or Excel file containing the keyword.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current_files = os.listdir('.')\n",
    "        matches = [f for f in current_files if keyword.lower() in f.lower() and (f.endswith('.csv') or f.endswith('.xls') or f.endswith('.xlsx'))]\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error scanning directory: {e}\")\n",
    "    return None\n",
    "\n",
    "def read_file_smartly(file_path):\n",
    "    \"\"\"\n",
    "    Tries multiple strategies to read the file, prioritizing Excel for .xls files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Check if it's likely a binary Excel file (.xls)\n",
    "    if file_path.lower().endswith('.xls'):\n",
    "        try:\n",
    "            # Try reading as Excel using xlrd (Required for .xls)\n",
    "            return pd.read_excel(file_path, engine='xlrd')\n",
    "        except ImportError:\n",
    "            print(f\"\\n   ‚ùå ERROR: You are missing the 'xlrd' library required for .xls files.\")\n",
    "            print(f\"      Please run: pip install xlrd\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # If it fails, maybe it's a CSV named as .xls? Fall through to CSV logic.\n",
    "            print(f\"   ‚ö†Ô∏è Excel read failed ({e}), attempting CSV read...\")\n",
    "\n",
    "    # 2. Strategy: Smart Separator Hunt (For CSVs)\n",
    "    separators = [',', '\\t', ';', '|']\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
    "    \n",
    "    for sep in separators:\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=sep, encoding=encoding, on_bad_lines='skip')\n",
    "                \n",
    "                # VALIDATION: Check for Binary Garbage\n",
    "                # If columns contain weird characters like '√∞√Ø', it's a binary file read as text. Reject it.\n",
    "                col_str = \"\".join(list(df.columns))\n",
    "                if \"\\ufffd\" in col_str or \"√∞\" in col_str or \"√†\" in col_str:\n",
    "                     continue # Skip this result, it's garbage\n",
    "                \n",
    "                if len(df.columns) > 1:\n",
    "                    return df\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # 3. Final Fallback: Try generic Excel (for .xlsx)\n",
    "    try:\n",
    "        return pd.read_excel(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n      [Debug] Failed to read '{file_path}'.\")\n",
    "        raise e\n",
    "\n",
    "def convert_excel_to_json():\n",
    "    all_records = []\n",
    "    print(\"üöÄ Starting Smart Conversion Process (Robust Mode with Verification)...\")\n",
    "    print(f\"üìÇ Scanning current folder: {os.getcwd()}\")\n",
    "    \n",
    "    # Validation Stats\n",
    "    stats = {}\n",
    "\n",
    "    for config in FILES_CONFIG:\n",
    "        system_name = config[\"system\"]\n",
    "        keyword = config[\"keyword\"]\n",
    "        mapping = config[\"mapping\"]\n",
    "        stats[system_name] = {\"total\": 0, \"unknown_terms\": 0}\n",
    "        \n",
    "        # 1. Find the file\n",
    "        file_path = find_file_by_keyword(keyword)\n",
    "        \n",
    "        if not file_path:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {system_name}: No file found with keyword '{keyword}'\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"üìñ Processing {system_name} (Found: {file_path})...\")\n",
    "        \n",
    "        try:\n",
    "            # 2. Read File (Smart Mode)\n",
    "            df = read_file_smartly(file_path)\n",
    "            \n",
    "            # 3. NORMALIZE HEADERS (Fixes \"Unknown\" issues)\n",
    "            # Convert all columns to lowercase and strip spaces\n",
    "            df.columns = [str(c).lower().strip() for c in df.columns]\n",
    "            \n",
    "            # Debug: Print found columns to help user verify\n",
    "            print(f\"   [Debug] Found Columns: {list(df.columns)[:5]} ...\")\n",
    "            \n",
    "            # 4. Process Data\n",
    "            file_records = []\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                # Extract data safely using lowercase keys\n",
    "                term = str(row.get(mapping[\"term\"], \"Unknown\")).strip()\n",
    "                code = str(row.get(mapping[\"tm2_code\"], \"Unknown\")).strip()\n",
    "                \n",
    "                # Check for bad mapping\n",
    "                if term == \"Unknown\" or term == \"nan\":\n",
    "                    stats[system_name][\"unknown_terms\"] += 1\n",
    "                    term = \"Unknown\" \n",
    "\n",
    "                # English fallback logic\n",
    "                english_val = row.get(mapping[\"english\"], None)\n",
    "                if pd.isna(english_val) or str(english_val).strip() == \"\":\n",
    "                    english = term\n",
    "                else:\n",
    "                    english = str(english_val).strip()\n",
    "                \n",
    "                record = {\n",
    "                    \"term\": term,\n",
    "                    \"english\": english,\n",
    "                    \"tm2_code\": code,\n",
    "                    \"system\": system_name\n",
    "                }\n",
    "                file_records.append(record)\n",
    "            \n",
    "            all_records.extend(file_records)\n",
    "            stats[system_name][\"total\"] = len(file_records)\n",
    "            print(f\"   ‚úÖ Added {len(file_records)} records from {system_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Critical Error processing {system_name}: {e}\")\n",
    "\n",
    "    # 5. Save Final JSON\n",
    "    if all_records:\n",
    "        with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_records, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        print(\"\\n===================================================\")\n",
    "        print(\"üîç QUALITY CHECK REPORT\")\n",
    "        print(\"===================================================\")\n",
    "        for sys, data in stats.items():\n",
    "            total = data[\"total\"]\n",
    "            unknowns = data[\"unknown_terms\"]\n",
    "            if total == 0:\n",
    "                print(f\"‚ùå {sys}: No records found.\")\n",
    "            elif unknowns == total:\n",
    "                print(f\"‚ö†Ô∏è  {sys}: {total} records, but ALL have 'Unknown' terms. (Check Headers!)\")\n",
    "            elif unknowns > 0:\n",
    "                print(f\"‚ö†Ô∏è  {sys}: {total} records, {unknowns} terms missing/unknown.\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {sys}: {total} records, 100% Clean.\")\n",
    "        \n",
    "        print(\"===================================================\")\n",
    "        print(f\"üéâ SUCCESS! Total {len(all_records)} records saved to '{OUTPUT_JSON_FILE}'\")\n",
    "        print(\"üëâ You can now run 'server.py' to use this database.\")\n",
    "    else:\n",
    "        print(\"‚ùå No records were processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_excel_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ffcced-a8c2-4730-a751-8acea1c3f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Smart Conversion (With Descriptions)...\n",
      "üìñ Processing Ayurveda...\n",
      "   ‚úÖ Added 2910 records.\n",
      "üìñ Processing Siddha...\n",
      "   ‚úÖ Added 1926 records.\n",
      "üìñ Processing Unani...\n",
      "   ‚úÖ Added 2522 records.\n",
      "üìñ Processing ICD-10...\n",
      "   ‚úÖ Added 11145 records.\n",
      "üéâ SUCCESS! 18503 records saved with descriptions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "OUTPUT_JSON_FILE = \"namaste_data.json\"\n",
    "\n",
    "# UPDATED: Added mapping for 'description' columns\n",
    "FILES_CONFIG = [\n",
    "    {\n",
    "        \"system\": \"Ayurveda\",\n",
    "        \"keyword\": \"AYURVEDA\", \n",
    "        \"mapping\": {\n",
    "            \"term\": \"namc_term\",          \n",
    "            \"english\": \"name english\",    \n",
    "            \"tm2_code\": \"namc_code\",\n",
    "            \"desc_long\": \"long_definition\",  # <--- New\n",
    "            \"desc_short\": \"short_definition\" # <--- New\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"Siddha\",\n",
    "        \"keyword\": \"SIDDHA\",\n",
    "        \"mapping\": {\n",
    "            \"term\": \"namc_term\",\n",
    "            \"english\": \"short_definition\", \n",
    "            \"tm2_code\": \"namc_code\",\n",
    "            \"desc_long\": \"long_definition\",\n",
    "            \"desc_short\": \"short_definition\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"Unani\",\n",
    "        \"keyword\": \"UNANI\",\n",
    "        \"mapping\": {\n",
    "            \"term\": \"numc_term\",\n",
    "            \"english\": \"short_definition\", \n",
    "            \"tm2_code\": \"numc_code\",\n",
    "            \"desc_long\": \"long_definition\",\n",
    "            \"desc_short\": \"short_definition\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"ICD-10\",\n",
    "        \"keyword\": \"ICD10\",\n",
    "        \"mapping\": {\n",
    "            \"term\": \"namc_term\",\n",
    "            \"english\": \"namc_term\",       \n",
    "            \"tm2_code\": \"namc_code\",\n",
    "            \"desc_long\": \"block_title\",      # Using block title as context\n",
    "            \"desc_short\": \"chapt_name\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def find_file_by_keyword(keyword):\n",
    "    try:\n",
    "        current_files = os.listdir('.')\n",
    "        matches = [f for f in current_files if keyword.lower() in f.lower() and (f.endswith('.csv') or f.endswith('.xls') or f.endswith('.xlsx'))]\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error scanning directory: {e}\")\n",
    "    return None\n",
    "\n",
    "def read_file_smartly(file_path):\n",
    "    # 1. Check if it's likely a binary Excel file (.xls)\n",
    "    if file_path.lower().endswith('.xls'):\n",
    "        try:\n",
    "            return pd.read_excel(file_path, engine='xlrd')\n",
    "        except ImportError:\n",
    "            print(f\"\\n   ‚ùå ERROR: Missing 'xlrd'. pip install xlrd>=2.0.1\")\n",
    "            raise\n",
    "        except Exception:\n",
    "            pass # Try CSV fallback\n",
    "\n",
    "    # 2. Strategy: Smart Separator Hunt (For CSVs)\n",
    "    separators = [',', '\\t', ';', '|']\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
    "    \n",
    "    for sep in separators:\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=sep, encoding=encoding, on_bad_lines='skip')\n",
    "                col_str = \"\".join(list(df.columns))\n",
    "                if \"\\ufffd\" in col_str or \"√∞\" in col_str or \"√†\" in col_str: continue \n",
    "                if len(df.columns) > 1: return df\n",
    "            except: continue\n",
    "\n",
    "    # 3. Final Fallback: Try generic Excel (for .xlsx)\n",
    "    try:\n",
    "        return pd.read_excel(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n      [Debug] Failed to read '{file_path}'.\")\n",
    "        raise e\n",
    "\n",
    "def clean_text(val):\n",
    "    \"\"\"Helper to clean up text values (remove NaNs, dashes, whitespace)\"\"\"\n",
    "    if pd.isna(val): return \"\"\n",
    "    val = str(val).strip()\n",
    "    if val in [\"-\", \"nan\", \"Unknown\", \"\"]: return \"\"\n",
    "    return val\n",
    "\n",
    "def convert_excel_to_json():\n",
    "    all_records = []\n",
    "    print(\"üöÄ Starting Smart Conversion (With Descriptions)...\")\n",
    "    \n",
    "    stats = {}\n",
    "\n",
    "    for config in FILES_CONFIG:\n",
    "        system_name = config[\"system\"]\n",
    "        keyword = config[\"keyword\"]\n",
    "        mapping = config[\"mapping\"]\n",
    "        stats[system_name] = {\"total\": 0}\n",
    "        \n",
    "        file_path = find_file_by_keyword(keyword)\n",
    "        if not file_path:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {system_name}: File not found.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"üìñ Processing {system_name}...\")\n",
    "        \n",
    "        try:\n",
    "            df = read_file_smartly(file_path)\n",
    "            # Normalize headers\n",
    "            df.columns = [str(c).lower().strip() for c in df.columns]\n",
    "            \n",
    "            file_records = []\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                term = clean_text(row.get(mapping[\"term\"]))\n",
    "                code = clean_text(row.get(mapping[\"tm2_code\"]))\n",
    "                \n",
    "                # English Logic\n",
    "                english = clean_text(row.get(mapping[\"english\"]))\n",
    "                if not english: english = term\n",
    "                \n",
    "                # --- NEW: Description Logic ---\n",
    "                # Try Long Definition first, then Short, then Default\n",
    "                desc_long = clean_text(row.get(mapping.get(\"desc_long\")))\n",
    "                desc_short = clean_text(row.get(mapping.get(\"desc_short\")))\n",
    "                \n",
    "                final_desc = \"\"\n",
    "                if desc_long: \n",
    "                    final_desc = desc_long\n",
    "                elif desc_short:\n",
    "                    final_desc = desc_short\n",
    "                else:\n",
    "                    final_desc = \"No description available.\"\n",
    "                \n",
    "                # Fallback for ICD-10 context\n",
    "                if system_name == \"ICD-10\" and final_desc:\n",
    "                     final_desc = f\"Category: {final_desc}\"\n",
    "\n",
    "                if not term: term = \"Unknown\"\n",
    "                if not code: code = \"Unknown\"\n",
    "\n",
    "                record = {\n",
    "                    \"term\": term,\n",
    "                    \"english\": english,\n",
    "                    \"tm2_code\": code,\n",
    "                    \"system\": system_name,\n",
    "                    \"description\": final_desc  # <--- Field added\n",
    "                }\n",
    "                file_records.append(record)\n",
    "            \n",
    "            all_records.extend(file_records)\n",
    "            stats[system_name][\"total\"] = len(file_records)\n",
    "            print(f\"   ‚úÖ Added {len(file_records)} records.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "    if all_records:\n",
    "        with open(OUTPUT_JSON_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_records, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"üéâ SUCCESS! {len(all_records)} records saved with descriptions.\")\n",
    "    else:\n",
    "        print(\"‚ùå No records processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_excel_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d1ad6-beb2-4462-8589-a2074d18439b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
